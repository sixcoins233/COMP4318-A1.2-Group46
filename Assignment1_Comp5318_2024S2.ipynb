{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP4318 & 5318 - Machine Learning and Data Mining: Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due: Sunday Week 7 - Sep 15th, 2024 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "375753da-1c6c-4b02-986a-6e3b185a5869"
    }
   },
   "source": [
    "# 1. Summary\n",
    "\n",
    "In this assignment, you are tasked with the challenge of developing machine learning (ML) classifiers capable of categorizing grayscale images into predefined classes. Your task involves employing various classification algorithms to identify which is most effective and efficient in processing image data. Additionally, you are required to document your methodologies and findings in a detailed report. The total score for this assignment is allocated as follows:\n",
    "\n",
    "1. Code: max 65 points\n",
    "2. Report: max 35 points\n",
    "\n",
    "Detailed about assignment specifcations and scoring criteria can be found in the assignment page on Canvas (Assignments $\\rightarrow$ Assignment 1 - Specification). The sections below provide comprehensive information on the assignment tasks and guidelines for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this assignment is derived from the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a collection of fashion articles represented as grayscale images. This dataset can be downloaded from Canvas. The dataset consists of a training set of 30,000 examples and a test set of 5,000 examples. They belong to 10 different categories. The validation set is not provided, but you can randomly pick a subset of the training set for validation. Your prediction over the test set must be submitted to Kaggle to receive the public accuracy over first 2,000 examples, you will use this accuracy to analyse the performance of your proposed method. It is NOT allowed to use any examples from the test set for training; or it will be considered as cheating. The rest 3,000 labels of the test set are reserved for marking purpose.\n",
    "\n",
    "Data samples are categorized into the following ten classes:\n",
    "\n",
    "- 0: T-shirt/Top\n",
    "- 1: Trouser\n",
    "- 2: Pullover\n",
    "- 3: Dress\n",
    "- 4: Coat\n",
    "- 5: Sandal\n",
    "- 6: Shirt\n",
    "- 7: Sneaker\n",
    "- 8: Bag\n",
    "- 9: Ankle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below are visual examples of the dataset, showcasing samples from each category:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/publication/346405197/figure/fig3/AS:962581560848384@1606508736352/Examples-of-Fashion-MNIST-dataset.ppm\" alt=\"DataSet\" title=\"DataSet\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded from the Assignment 1 page on Canvas. Note that only a subset of the original Fashion-MNIST dataset is provided for this assignment. You must use the specific files supplied in the assignment materials for training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required `data` files are in the data folder, downloadable as a zip from the Assignment 1 - Specification page on Canvas. Extract the files into your working directory. The folder includes:\n",
    "\n",
    "- `train.csv`: 30,000 labeled samples for training, evaluation, and model selection.\n",
    "- `test1.csv`: 2,000 labeled samples for model efficiency testing (Canvas submission).\n",
    "- `test2.csv`: 5,000 unlabeled samples for Kaggle evaluation.\n",
    "- `sample.csv`: A sample prediction file format for Kaggle submission (`test_output.csv`).\n",
    "\n",
    "Use Python's pandas library to load these CSV files into DataFrames, ensuring they are under the `./data/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following Python code to load the training data:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "print(os.listdir(\"./data\"))\n",
    "pd.set_option('display.max_columns', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.csv including feature and label using for training model.\n",
    "data_train_df = pd.read_csv('./data/train.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first 5 rows of the training dataframe\n",
    "data_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then data would be a dataframe with 30,000 samples including 784 features (from v1 to v784) and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting input feature\n",
    "# image 28*28 = 784\n",
    "data_train_feature = data_train_df.loc[:, \"v1\":\"v784\"].to_numpy()\n",
    "\n",
    "# Selecting output lable \n",
    "data_train_label = data_train_df.label.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing a sample data. The first example belongs to class 2: Pullover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data_train_feature = data_train_feature.reshape((data_train_feature.shape[0], 28, 28))\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(data_train_feature[0], cmap=plt.get_cmap('gray'))\n",
    "plt.title(\"class \" + str(data_train_label[0]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1e4a01db-cd92-48f8-bdaa-21c39456cfcb"
    }
   },
   "source": [
    "# 4. Task Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now explore and build diffferent ML models for the given dataset. You are required to implement at least **FOUR** models, which should include THREE from the following methods:\n",
    "\n",
    "1. Nearest Neighbor\n",
    "2. Logistic Regression\n",
    "3. Na√Øve Bayes \n",
    "4. Decision Tree\n",
    "5. SVM\n",
    "\n",
    "and ONE of these ensemble methods:\n",
    "    \n",
    " 1. Bagging\n",
    " 2. Boosting\n",
    " 3. Random forest\n",
    "    \n",
    "\n",
    "For these implementations, you may use established packages and libraries that have been introduced in the tutorials such as sklearn or create your own custom solutions from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code must be easily readable and well commented. The following are expected to be satisfied:\n",
    "\n",
    "- **Readability & Consistency**: Easy to read, and consistent in style\n",
    "  \n",
    "- **Coding Descriptions & Comments**: Descriptions and comments clarify meaning where needed\n",
    "  \n",
    "- **Robustness**: Handles erroneous or unexpected input \n",
    "\n",
    "It should follow the structure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and import necessary packages and libraries used in your coding environment. It is recommended to specify their versions to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Install and import necessary libraries\n",
    "\n",
    "# For preprocessing like Normalization and Dimensionality Reduction\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For model 1 using KNN (Nearest neighbors model)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# For model 2 using Naive bayes\n",
    "\n",
    "# For model 3 using SVM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define any necessary utility or helper functions (e.g., for plotting, optimization, etc.) if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define helper function (e.g. plotting) if applicable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_cumulative_variance_ratio(X, n_components=None):\n",
    "    \"\"\"\n",
    "    Draw a PCA diagram to explain the variance ratio.\n",
    "\n",
    "    Args:\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        \n",
    "    n_components : int, optional\n",
    "        The Main components needs to calculate (when to stop), None is default\n",
    "\n",
    "    Return:\n",
    "        int(average number of 95% and 99% components)\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "    \n",
    "    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'bo-')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('Cumulative Explained Variance Ratio vs. Number of PCA Components')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add a line to shows the 95% and 99% variance\n",
    "    plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "    plt.axhline(y=0.99, color='g', linestyle='--', label='99% Explained Variance')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # print the result to let user select which value they want to use\n",
    "    components_95 = next(i for i, ratio in enumerate(cumulative_variance_ratio) if ratio >= 0.95) + 1\n",
    "    components_99 = next(i for i, ratio in enumerate(cumulative_variance_ratio) if ratio >= 0.99) + 1\n",
    "    \n",
    "    print(f\"need {components_95} components to explain the 95% of variance\")\n",
    "    print(f\"need {components_99} components to explain the 99% of variance\")\n",
    "\n",
    "    return int((components_95 + components_99) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement at least ONE preprocessing technique on the dataset before model training. Possible methods include **Normalization**, **Dimensionality Reduction**, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement Preprocessing Techniques\n",
    "\n",
    "# Reload feature and label Since avoid \n",
    "data_train_feature = data_train_df.loc[:, \"v1\":\"v784\"].to_numpy()\n",
    "data_train_label = data_train_df.label.to_numpy()\n",
    "\n",
    "# split the train and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_train_feature, data_train_label, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalization: Transforms the range of features to a standard scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Take a look for how many components should be set\n",
    "components = plot_cumulative_variance_ratio(X_train_scaled)\n",
    "print(f\"Using components {components} for PCA\")\n",
    "\n",
    "# Dimensionality Reduction\n",
    "pca = PCA(n_components=components)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the initial version of your model using a set of predefined hyperparameters. This will establish a baseline from which improvements can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement model 1 KNN\n",
    "\n",
    "# KNN hyperparameters \n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "\n",
    "# predict the split test value\n",
    "y_pred = knn.predict(X_test_pca)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"accuracy of test set: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhance your model by fine-tuning its hyperparameters. Use techniques such as grid search combined with k-fold cross-validation to systematically identify the optimal parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fine-tune the hyperparameters of model 1\n",
    "\n",
    "# Data preprocessing and knn define in pipeline for grid_search\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),             # Scaler, StanderdScaler\n",
    "    ('pca', PCA(n_components=components)),    # PCA, Dimension reduction\n",
    "    ('knn', KNeighborsClassifier())           # KNN model\n",
    "])\n",
    "\n",
    "# Define the hyperparameters for neighbors number, weights method, distence metric algorithm method\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': [3, 9, 11, 17, 23, 57, 127],     # KNN K-neighbors\n",
    "    'knn__weights': ['uniform', 'distance'],                 # KNN weight method\n",
    "    'knn__metric': ['euclidean', 'manhattan']                # distence metric method\n",
    "}\n",
    "\n",
    "# grid search with cross validation\n",
    "# cv : Cross Validation number\n",
    "# scoring : which value we focus for correctness of model\n",
    "# n_jobs : how many CPU thread using, default -1 means using all threads\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"Start Grid Search\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Hyperparameter setting:\", grid_search.best_params_)\n",
    "print(\"Best cross validation score:\", grid_search.best_score_)\n",
    "\n",
    "# See the best model result\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Fine-tuned model's accuracy rate in testset':\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and fine-tune the hyperparameters for Model 2 (using the same approach as Model 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fine-tune the hyperparameters for model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.5 Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and fine-tune the hyperparameters for Model 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fine-tune the hyperparameters for model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.6 Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement and fine-tune the hyperparameters for Model 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fine-tune the hyperparameters for model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.7 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the best version of each model using appropriate classification performance metrics on the validation set and test on `test1.csv`. Ensure that the results are visualized using high-quality plots, figures, or tables to clearly demonstrate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.8 Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare all classifiers with their optimized hyper-parameters, focusing on criteria such as classification performance, training time, and inference time. Visualization of these comparisons is required; use high-quality plots, figures, or tables to facilitate a clear understanding of the differences and strengths of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare performance of all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.9 The Best Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclude the best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train and test the classifier which has the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.10 Loading testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing data for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test2.csv includes 5000 samples used for label prediction. Test samples do not have labels.\n",
    "data_test_df = pd.read_csv('./data/test2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the first 5 rows of the test dataframe\n",
    "data_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the your best classifier to make predictions for the test data. The predictions should be stored in a vector named `output`, with a length of 5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use your best classifier to make predictions on unseen data. The output of this code must be a vector named 'output' of length 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your prediction vector as a `test_output.csv` file, which contains two columns: `id` and `label`. Please refer to the `example_output.csv` for the structure of this output file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(output, columns = ['label'])\n",
    "output_df.to_csv('./test_output.csv', sep=\",\", float_format='%d',index_label=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The report must be structured into the following key sections:\n",
    "\n",
    "1. **Introduction**: Provide a comprehensive overview of the dataset, outline the methods chosen, and summarize the key findings and results.\n",
    "2. **Methodology**: Describe pre-processing techniques and ML algorithms employed in this assignment. Include a discussion of the theoretical principles underlying each method and explain the rationale behind your choices.\n",
    "3. **Result and Discussion**: Detail the experimental settings (e.g., implementation strategies, hyperparameter finetuning strategies, etc.). Present the results obtained from the selected algorithms and discuss their implications. Compare the performance of all models, considering factors such as accuracy, model complexity, training time, and inference time. Employ high-quality plots, figures, and tables to visually support and enhance the discussion of these results.\n",
    "4. **Conclusion**: Summarize your main findings, mention any limitations methods and results and suggest potential directions for future works.\n",
    "5. **References**: include the references cited in your report in a consistent format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Important Notes\n",
    "\n",
    "- The maximum length of the main report is 8 pages (excluding appendix and references).\n",
    "\n",
    "-  You must include an appendix that clearly provides the instructions on how to setup the environment to run your code, especialy the installation guide and version of any external packages and\n",
    "libraries used for implementation. In addition, you should include the hardware configurations used for the coding environment.\n",
    "\n",
    "- The report must be in PDF format. Make sure the report is well-structured, easy to read, and that it presents your findings in a logical and organized way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Submission Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Group Registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, you can work in groups of TWO. Please register your group under *People ‚Üí Group ‚Üí A1.1-Group* or *People ‚Üí Group ‚Üí A1.2-Group* on Canvas (We have created two separate group sets to accommodate the large number of students enrolled in this course).  \n",
    "\n",
    "**The group registration should be done by Friday, Aug 30th, 2024.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Submit your work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Submit to Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Kaggle leaderboard for evaluating the results predicted by your models on unseen data. Follow the steps below to submit your work to the Kaggle leaderboard.\n",
    "\n",
    "Kaggle link: [https://www.kaggle.com/competitions/comp-4318-5318-2024-s-2-a-1/](https://www.kaggle.com/competitions/comp-4318-5318-2024-s-2-a-1/)\n",
    "\n",
    "1. Use the [Kaggle link](https://www.kaggle.com/competitions/comp-4318-5318-2024-s-2-a-1/) to join the competition, you need to create a Kaggle account if you don‚Äôt\n",
    "have one.\n",
    "\n",
    "2. Go to Team $\\rightarrow$ Use your registered Group ID on Canvas as your team name (e.g., `A1.1-Group 1`). You can create a Kaggle team with up to 2 members.\n",
    "   \n",
    "3. Go to Description $\\rightarrow$ Check the IMPORTANT NOTES for the assignment.\n",
    "   \n",
    "4. Submit Predictions $\\rightarrow$ Follow the submission format and submit your prediction output file (`test_output.csv`).\n",
    "   \n",
    "5. Leaderboard $\\rightarrow$ Check your accuracy score at the Leaderboard.\n",
    "\n",
    "In summary, go to [Kaggle Page](https://www.kaggle.com/competitions/comp-4318-5318-2024-s-2-a-1/) $\\rightarrow$ Join Competition $\\rightarrow$ Create a Team $\\rightarrow$ Submit Predictions $\\rightarrow$ Submit file `test_output.csv`\n",
    "\n",
    "IMPORTANT: This link is only available to the students of COMP4318/5318. All groups need to submit `test_output.csv` to Kaggle for marking puporse. Only 5 submissions are allowed per day\n",
    "for Kaggle. Group ID on Canvas and Kaggle have to be identical otherwise the submission will not be marked for the Accuracy part.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Submit to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proceed to the submission box on Canvas and submit 4 files separately as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   1. A `.pdf` report file.\n",
    "\n",
    "   2. An `.ipynb` code file: a Jupyter Notebook containing all your implementation. You can reuse the provided `.ipynb` template.\n",
    "    \n",
    "   3. A `.pdf` code file: this file is exported from the `.ipynb` file for checking plagiarism. \n",
    "\n",
    "   4. A `test_output.csv` file: contains the predictions made by your best classifier on unseen data. This file must be consistent with the one submitted on Kaggle.\n",
    "\n",
    "There are two different submission boxes for the different group sets: *Assignment 1 - Submission (for A1.1-Group)* and *Assignment 1 - Submission (for A1.2-Group)*. Please ensure you submit to the correct box corresponding to your group ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File Naming Conventions \n",
    "\n",
    "The submission files should be named with your group ID and all student ID (SID) separated by the underscore (_). For example,\n",
    "\n",
    "- a1_groupID_SID1_SID2.ipynb (code) \n",
    "  \n",
    "- a1_groupID_SID1_SID2.pdf (pdf version of the code)\n",
    "  \n",
    "- a1_groupID_SID1_SID2_report.pdf (report)\n",
    "  \n",
    "where SID1 and SID2 are the SIDs of the two students.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Important Notes:\n",
    "-\tOnly one group member needs to submit the assignment on behalf of the group.\n",
    "\n",
    "-\tDo NOT submit the dataset or zip files to Canvas. We will copy the `data` folder to the same directory with your `.ipynb` file to run your code. Please make sure your code is able to read the dataset from this folder.\n",
    "\n",
    "-\tBoth the code and report will be checked for plagiarism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other guidelines\n",
    "\n",
    "1.\tPlease refer to lecture notes, lab materials, and other course resources for different ML methods. \n",
    "\n",
    "2.\tPlease proceed your own way if we do not specify it in the assignment details.\n",
    "\n",
    "3.\tYou can use any packages or code which have been introduced in lectures or tutorials. If you use any other packages or code snippets, please put the reference at the bottom of the code. Otherwise, it will be considered as plagiarism and the relevant section will not be marked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Late Submission Penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A penalty of MINUS 5 percent (-5%) for each day after the due date. \n",
    "The maximum delay for assignment submission is 5 (five) days, after which assignment will not be accepted.\n",
    "\n",
    "**You should upload your assignment at least half a day or one day prior to the submission deadline to avoid network congestion**.\n",
    "\n",
    "Canvas and Kaggle may not be able to handle a large number of submission happening at the same time. If you submit your assignment at a time close to the deadline, a submission error may occur causing your submission to be considered late. Penalty will be applied to late submission regardless of issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All files required for assignment 1 can be downloaded from Canvas $\\rightarrow$ Assignments $\\rightarrow$ Assignment 1 - Specification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Marking Rubric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the rubric, which is available in the submission boxes on Canvas, for detailed marking scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Inquiries after releasing the marking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After Assignment 1 marks come out, please submit your inquiries about marking within the 1st week. All inquiries after that will be ignored.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Academic honesty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please read the University policy on Academic Honesty very carefully: \n",
    "https://sydney.edu.au/students/academic-integrity.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plagiarism (copying from another student, website or other sources), making your work available to another student to copy, engaging another person to complete the assignments instead of you (for payment or not) are all examples of academic dishonesty. Note that when there is copying between students, both students are penalised ‚Äì the student who copies and the student who makes his/her work available for copying. The University penalties are severe and include: \n",
    "\n",
    "    * a permanent record of academic dishonesty on your student file, \n",
    "    * mark deduction, ranging from 0 for the assignment to Fail for the course\n",
    "    * expulsion from the University and cancelling of your student visa. \n",
    "\n",
    "In addition, the Australian Government passed a new legislation last year (Prohibiting Academic Cheating Services Bill) that makes it a criminal offence to provide or advertise academic cheating services - the provision or undertaking of work for students which forms a substantial part of a student‚Äôs assessment task. Do not confuse legitimate co-operation and cheating!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
